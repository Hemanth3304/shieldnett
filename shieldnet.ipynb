{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQTvPRnGWw/KNmkS1EBzBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemanth3304/shieldnett/blob/main/shieldnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSC9m19vfukJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path)\n",
        "    print(\"‚úÖ Dataset Loaded Successfully!\")\n",
        "    print(f\"üî¢ Shape: {df.shape}\")\n",
        "    print(\"üßæ First 5 entries:\")\n",
        "    print(df.head())\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\\\S+|www\\\\S+|https\\\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\\\s]\", \"\", text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def preprocess_dataframe(df, text_column=\"text\"):\n",
        "    df[text_column] = df[text_column].astype(str).apply(clean_text)\n",
        "    return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsoLR-Emf7ty",
        "outputId": "291827b9-0477-4b10-8995-3a0c5c3914df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def extract_features(df, text_column=\"text\", label_column=\"label\", method=\"tfidf\"):\n",
        "    X = df[text_column]\n",
        "    y = df[label_column]\n",
        "\n",
        "    if method == \"tfidf\":\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    elif method == \"bow\":\n",
        "        vectorizer = CountVectorizer(max_features=5000)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid method: choose 'tfidf' or 'bow'\")\n",
        "\n",
        "    X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_vectorized, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, vectorizer\n"
      ],
      "metadata": {
        "id": "evxezz7qgJEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "def train_models(X_train, y_train):\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "        \"SVM\": SVC(kernel='linear', probability=True),\n",
        "        \"RandomForest\": RandomForestClassifier(),\n",
        "        \"NaiveBayes\": MultinomialNB()\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        print(f\"‚úÖ Trained {name}\")\n",
        "\n",
        "    return models\n"
      ],
      "metadata": {
        "id": "Kb8WqPuvgMd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"üß™ Evaluation Results:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "oNblbGyqgTBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "df = load_dataset(\"/content/Suspicious Communication on Social Platforms.csv\")\n",
        "df = preprocess_dataframe(df, text_column=\"comments\")\n",
        "\n",
        "# Extract features\n",
        "X_train, X_test, y_train, y_test, _ = extract_features(df, text_column=\"comments\", label_column=\"tagging\", method=\"tfidf\")\n",
        "\n",
        "# Train models\n",
        "models = train_models(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüìä Evaluating {name}\")\n",
        "    evaluate_model(model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ0HMI_RmCCF",
        "outputId": "a2caefb3-6754-4c16-bf44-b3eba12f1e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset Loaded Successfully!\n",
            "üî¢ Shape: (20001, 2)\n",
            "üßæ First 5 entries:\n",
            "                                            comments  tagging\n",
            "0                             Get fucking real dude.        1\n",
            "1   She is as dirty as they come  and that crook ...        1\n",
            "2   why did you fuck it up. I could do it all day...        1\n",
            "3   Dude they dont finish enclosing the fucking s...        1\n",
            "4   WTF are you talking about Men? No men thats n...        1\n",
            "‚úÖ Trained LogisticRegression\n",
            "‚úÖ Trained SVM\n",
            "‚úÖ Trained RandomForest\n",
            "‚úÖ Trained NaiveBayes\n",
            "\n",
            "üìä Evaluating LogisticRegression\n",
            "üß™ Evaluation Results:\n",
            "Accuracy: 0.8517870532366908\n",
            "Precision: 0.9939759036144579\n",
            "Recall: 0.6277742549143944\n",
            "F1 Score: 0.769529731830548\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89      2424\n",
            "           1       0.99      0.63      0.77      1577\n",
            "\n",
            "    accuracy                           0.85      4001\n",
            "   macro avg       0.90      0.81      0.83      4001\n",
            "weighted avg       0.88      0.85      0.84      4001\n",
            "\n",
            "\n",
            "üìä Evaluating SVM\n",
            "üß™ Evaluation Results:\n",
            "Accuracy: 0.970757310672332\n",
            "Precision: 0.994579945799458\n",
            "Recall: 0.9308814204185162\n",
            "F1 Score: 0.9616770389780543\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      2424\n",
            "           1       0.99      0.93      0.96      1577\n",
            "\n",
            "    accuracy                           0.97      4001\n",
            "   macro avg       0.98      0.96      0.97      4001\n",
            "weighted avg       0.97      0.97      0.97      4001\n",
            "\n",
            "\n",
            "üìä Evaluating RandomForest\n",
            "üß™ Evaluation Results:\n",
            "Accuracy: 0.9702574356410897\n",
            "Precision: 0.994572591587517\n",
            "Recall: 0.9296131896005073\n",
            "F1 Score: 0.9609963946247132\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      2424\n",
            "           1       0.99      0.93      0.96      1577\n",
            "\n",
            "    accuracy                           0.97      4001\n",
            "   macro avg       0.98      0.96      0.97      4001\n",
            "weighted avg       0.97      0.97      0.97      4001\n",
            "\n",
            "\n",
            "üìä Evaluating NaiveBayes\n",
            "üß™ Evaluation Results:\n",
            "Accuracy: 0.8500374906273431\n",
            "Precision: 0.9949341438703141\n",
            "Recall: 0.6227013316423589\n",
            "F1 Score: 0.765990639625585\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89      2424\n",
            "           1       0.99      0.62      0.77      1577\n",
            "\n",
            "    accuracy                           0.85      4001\n",
            "   macro avg       0.90      0.81      0.83      4001\n",
            "weighted avg       0.88      0.85      0.84      4001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlumahsgmB-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(models[\"RandomForest\"], \"random_forest_model.pkl\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTs_lR49hwwd",
        "outputId": "397f9616-3205-446b-9cf4-ea97de231d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# üß† TF-IDF + Train model\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['comments'])\n",
        "y = df['tagging']\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X, y)\n",
        "\n",
        "# üíæ Save model and vectorizer\n",
        "with open(\"cyberbully_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"‚úÖ Model and vectorizer saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr0VIgKVzkpz",
        "outputId": "03461fac-d991-42e0-844c-e59a20f120fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and vectorizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(model, vectorizer, input_text):\n",
        "    cleaned = clean_text(input_text)\n",
        "    vec = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vec)[0]\n",
        "    return \"Cyberbullying\" if prediction == 1 else \"Non-Cyberbullying\""
      ],
      "metadata": {
        "id": "mnO2m6duiG6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0212fd17"
      },
      "source": [
        "To mount your Google Drive, run the following code cell and follow the instructions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fa450b8"
      },
      "source": [
        "Once your Drive is mounted, you can access your files using the path `/content/drive/My Drive/your_folder/your_file.csv`. Please replace `your_folder/your_file.csv` with the actual path to your dataset file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter a sentence to test: \")\n",
        "print(predict_text(model, vectorizer, text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-IfvoCVhwQc",
        "outputId": "303c7832-4c8c-4f05-cb1a-bc677c60738b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence to test: good\n",
            "Non-Cyberbullying\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, vectorizer = extract_features(df, text_column='comments', label_column='tagging')"
      ],
      "metadata": {
        "id": "bL0cMlvSOoZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the vectorizer\n",
        "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
        "print(\"‚úÖ Vectorizer saved as 'vectorizer.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYFoimI_OoLR",
        "outputId": "a7146976-a0a9-49cf-9c8d-c4c19e451900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vectorizer saved as 'vectorizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import joblib\n",
        "\n",
        "def evaluate_all_models(models, X_test, y_test):\n",
        "    best_model = None\n",
        "    best_f1 = 0\n",
        "    best_name = \"\"\n",
        "\n",
        "    for name, model in models.items():\n",
        "        y_pred = model.predict(X_test)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        print(f\"\\nüìä {name} F1 Score: {f1:.4f}\")\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_model = model\n",
        "            best_name = name\n",
        "\n",
        "    print(f\"\\nüèÜ Best Model: {best_name} (F1 Score: {best_f1:.4f})\")\n",
        "    joblib.dump(best_model, \"best_model.pkl\")\n",
        "    print(\"‚úÖ Best model saved as 'best_model.pkl'\")\n",
        "\n",
        "    return best_model\n",
        "\n"
      ],
      "metadata": {
        "id": "bpegSowm_ypc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%writefile app.py\n",
        "import streamlit as st\n",
        "st.title(\"‚úÖ Hello from Streamlit!\")\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Load model and vectorizer\n",
        "model = joblib.load(\"random_forest_model.pkl\")\n",
        "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "\n",
        "st.title(\"üõ°Ô∏è Cyberbullying Comment Checker\")\n",
        "\n",
        "comment = st.text_area(\"üí¨ Enter your comment:\")\n",
        "\n",
        "if st.button(\"Check\"):\n",
        "    if comment.strip() == \"\":\n",
        "        st.warning(\"Please enter a comment.\")\n",
        "    else:\n",
        "        cleaned = clean_text(comment)\n",
        "        features = vectorizer.transform([cleaned])\n",
        "        prediction = model.predict(features)[0]\n",
        "\n",
        "        if prediction == 1:\n",
        "            st.error(\"üö´ This comment appears to be bullying. It cannot be posted.\")\n",
        "        else:\n",
        "            st.success(\"‚úÖ This comment is safe to post.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-USad9YNjvZ",
        "outputId": "c604711d-f969-42a0-ece0-3d04d39ca416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXT-ti-AQCMq",
        "outputId": "29f7ece8-b99a-4c08-c215-67a482c6b523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.45.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken  2zVg8VeaXkDIFhdrMD7VmcDoMyF_84cEV62jevg8r78YsK2GX\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5hY5K0aQJ6q",
        "outputId": "81c05810-671c-463b-ec2e-72e6d5de4994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "_kfyACCARy9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvQtErQUQSh6",
        "outputId": "640e7375-0083-4695-8700-2b81eb556b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.233.224.198:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from pyngrok import ngrok\n",
        "\n",
        "# Make sure ngrok is running on the same port Streamlit is using (8501)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåê Streamlit is live at:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spYTU7A6Qbmz",
        "outputId": "2401c2af-c109-447d-ac22-21c8d4f52e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Streamlit is live at: NgrokTunnel: \"https://2ffe-35-233-224-198.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab62caa1"
      },
      "source": [
        "# Find and kill all running ngrok processes\n",
        "!pkill ngrok\n",
        "\n",
        "# You can verify that no ngrok processes are running with:\n",
        "# !pgrep ngrok"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}